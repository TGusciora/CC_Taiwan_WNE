---
title: "Machine Learning 2"
subtitle: "4. Boosting"
author: "Paweł Sakowski"
date: "`r Sys.time()`"
output:
  html_document:  
    theme: spacelab
    highlight: tango
    css: "../slides/css/script.css"
    toc: true
    toc_float:
      collapsed: false
    smooth_scroll: true
    include:
      in_header:  "../slides/css/header2.html"
      after_body: "../slides/css/footer.html"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```


```{r }
# ############################################################################
# 	    Uniwersytet Warszawski, Wydzial Nauk Ekonomicznych                   #
#                                                                            #
#             S T U D I A      P O D Y P L O M O W E                         #
#                                                                            #
#            Data Science w zastosowaniach biznesowych.                      #
#              Warsztaty z wykorzystaniem programu R                         #
#                                                                            #
#                                                                            #
#                     rok akademicki 2019/2020                               #
#                        Machine learning 2                                  #
#                                                                            #
# ############################################################################
# 4. Drzewa decyzyjne: boosting                                              #
# ############################################################################
```


Wczytajmy do pamięci niezbędne pakiety

```{r}
library(gbm)
library(xgboost)
library(fastAdaboost)
# devtools::install_github("nivangio/adaStump")
library(adaStump)
library(caret)
library(dplyr)
library(tibble)
library(readr)
library(here)
```


# Przykład 4.1 algorytm GBM, zbiór `daneusa`

```{r}
# ######################################################
# Przykład 4.1 
# zbiór daneusa
# GBM
# ######################################################
```

Zastosowanie boostingu zilustrujemy na przykładzie problemu klasyfikacji. 
Dla problemu regresyjnego analiza przebiega jednek analogicznie 
(wskazujemy jedynie inny zakładany rozkład zmiennej).

Ładujemy dane:

```{r}
daneusa <- readRDS(here("data", "daneusa.rds"))
```

Definujemy formuły dwóch modeli. Jest to potrzebne już na tym etapie 
z uwagi na konieczność zbilansowania zbioru.

```{r}
model1.formula_01 <- UCURNINS_1 ~ UMARSTAT + USATMED + URELATE + REGION + 
  FHOSP + FDENT + FEMER + FDOCT + UIMMSTAT + 
  UAGE + U_FTPT + U_WKSLY + U_USHRS + 
  HOTHVAL + HRETVAL + HSSVAL + 
  UBRACE + UEDUC3 + GENDER
model1.formula    <- UCURNINS   ~ UMARSTAT + USATMED + URELATE + REGION + 
  FHOSP + FDENT + FEMER + FDOCT + UIMMSTAT + 
  UAGE + U_FTPT + U_WKSLY + U_USHRS + 
  HOTHVAL + HRETVAL + HSSVAL + 
  UBRACE + UEDUC3 + GENDER

```


Dzielimy zdane na zbiór treningowy i zbiór testowy 
```{r}
set.seed(123456789)
training_obs <- createDataPartition(daneusa$UCURNINS, 
                                    p = 0.7, 
                                    list = FALSE) 
daneusa.train <-
  ROSE::ovun.sample(formula = model1.formula, 
                    data = daneusa[training_obs,],
                    method = "both",
                    p = 0.5, 
                    N = length(training_obs),
                    seed = 123)[["data"]]

daneusa.test  <- daneusa[-training_obs,] %>% as.data.frame()
```

Aby przyspieszyć wykonywane obliczenia wybierzmy losowo 10000 obserwacji 
z oryginalnego zbioru treningowego.

```{r}
set.seed(123456789)
daneusa.train2 <-
  daneusa.train[sample(1:nrow(daneusa.train),
                       size = 10000,
                       replace = FALSE), ]
```

Trenujemy pierwszy model z arbitralnie ustalonymi wartościami parametrów.
Zastosujemy funkcję `gbm()`, w której opcja `distribution` decyduje o 
rozkładzie zmiennej zależnej:

* dla analizy regresji stosujemy rozkładu normalny, tj. `distribution = "gaussian"` 
* dla klasyfikacji binarnej `distribution = "bernoulli"`

Zmienna zależna musi być zakodowana jako `0/1`.

Utwórzmy zatem zmienną 0/1, gdzie 1 będzie oznaczało `"sukces"`, czyli 
w tym wypadku brak ubezpieczenia.


```{r}
# zarówno w próbie pełnej
daneusa.train$UCURNINS_1 <-
  (daneusa.train$UCURNINS == "Yes") * 1

# jak i w mniejszej 
daneusa.train2$UCURNINS_1 <-
  (daneusa.train2$UCURNINS == "Yes") * 1
```


Parametrami decydującymi o skuteczności procedury boostingu są:

1. `n.trees` - liczba iteracji, czyli drzew, które zostaną stworzone  
2. `interaction.depth` - głębokość drzew, czyli stopień ich złożoności, określany jako liczba podziałów w drzewie (liczona od pierwzego pojedynczego węzła)
3. `shrinkage`         - współczynnik *learning rate*
4. `n.minobsinnode`    – minimalna liczba obserwacji w węźle, aby wykonać podział

Za chwilę te parametry będziemy optymalizować. 

Póki co wybierzmy arbitralnie wartości parametrów:

```{r}
set.seed(123456789)
daneusa.gbm <- 
  gbm(model1.formula_01,
      data = daneusa.train,
      distribution = "bernoulli",
      # całkowita liczba drzew
      n.trees = 500,
      # liczba interakcji zmiennych - de facto głębokość drzewa
      interaction.depth = 4,
      # shrinkage parameter - szybkość uczenia
      shrinkage = 0.01,
      verbose = FALSE)
```


Zapiszemy od razu model do pliku `rds`

```{r}
daneusa.gbm %>% saveRDS(here("output", "daneusa.gbm.rds"))
```

Generujemy prognozę na zbiór treningowy:

```{r}
daneusa.pred.train.gbm <- predict(daneusa.gbm,
                                  daneusa.train, 
                                  # type = "response" daje 
                                  # w tym przypadku
                                  # prawdopodobieństwo sukcesu
                                  type = "response",
                                  # parametr n.trees umożliwia
                                  # wybranie iteracji, z której 
                                  # prognozę zwracamy
                                  n.trees = 500)
```


oraz testowy:

```{r}
daneusa.pred.test.gbm <- predict(daneusa.gbm,
                                 daneusa.test, 
                                 # type = "response" daje 
                                 # w tym przypadku
                                 # prawdopodobieństwo sukcesu
                                 type = "response",
                                 # parametr n.trees umożliwia
                                 # wybranie iteracji, z której 
                                 # prognozę zwracamy
                                 n.trees = 500)

```

Wykorzystajmy funkcję `getAccuracyAndGini2()`, która jako pierwszy argument 
bierze ramkę danych zawierającą zarówno wartości rzeczywiste zmiennej celu,
jak i prognozowane prawdopodonieństwa sukcesu. Funkcja ta, podobnie jak 
funkcja `getAccuracyAndGini()` wyświetla wartości accuracy, sensitivity, 
specificity oraz Gini.

```{r}
source(here("funs", "getAccuracyAndGini2.R"))
```

próba ucząca:
```{r}
getAccuracyAndGini2(data = data.frame(UCURNINS = daneusa.train$UCURNINS,
                                      pred = daneusa.pred.train.gbm),
                    predicted_probs = "pred",
                    target_variable = "UCURNINS") 
```

próba testowa:

```{r}
getAccuracyAndGini2(data = data.frame(UCURNINS = daneusa.test$UCURNINS,
                                      pred = daneusa.pred.test.gbm),
                    predicted_probs = "pred",
                    target_variable = "UCURNINS") 
```

Wynik w próbie testowej lepszy niż z lasów losowych!

Przedstawmy jeszcze krzywe ROC na wykresie

```{r}
ROC.train.gbm <- pROC::roc(daneusa.train$UCURNINS, 
                           daneusa.pred.train.gbm)
ROC.test.gbm  <- pROC::roc(daneusa.test$UCURNINS, 
                           daneusa.pred.test.gbm)
cat("AUC for train = ", pROC::auc(ROC.train.gbm), 
    ", Gini for train = ", 2 * pROC::auc(ROC.train.gbm) - 1, "\n", sep = "")
cat("AUC for test = ", pROC::auc(ROC.test.gbm), 
    ", Gini for test = ", 2 * pROC::auc(ROC.test.gbm) - 1, "\n", sep = "")
plot(ROC.train.gbm)
lines(ROC.test.gbm, col = "red")
```

Ładniej jednak będzie przy wykorzystaniu poprzedniego podejścia, 
czyli przy użyciu funkcji `ggroc()`.

```{r}
list(
  ROC.train.gbm = ROC.train.gbm,
  ROC.test.gbm  = ROC.test.gbm
) %>%
  pROC::ggroc(alpha = 0.5, linetype = 1, size = 1) + 
  geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1), 
               color = "grey", 
               linetype = "dashed") +
  labs(subtitle = paste0("Gini TRAIN: ",
                         "gbm = ", 
                         round(100 * (2 * auc(ROC.train.gbm) - 1), 1), "%, ",
                         "Gini TEST: ",
                         "gbm = ", 
                         round(100 * (2 * auc(ROC.test.gbm) - 1), 1), "%, "
  )) +
  theme_bw() + coord_fixed() +
  scale_color_brewer(palette = "Paired")
```

Widzimy dodatkowo, że model nie jest przeuczony!

Wykonajmy tuning i spróbujmy znaleźć optymalny zestaw parametrów, dorzucając 
do tego jeszcze minimalną liczebność w węźle.

```{r}
modelLookup("gbm")
parametry_gbm <- expand.grid(interaction.depth = c(1, 2, 4),
                             n.trees = c(100, 500),
                             shrinkage = c(0.01, 0.1), 
                             n.minobsinnode = c(100, 250, 500))
ctrl_cv3 <- trainControl(method = "cv", 
                         number = 3,
                         classProbs = TRUE,
                         summaryFunction = twoClassSummary)
```

Wykorzystajmy tu nieco mniejszą próbę ze zbioru `daneusa.train2`.

I tak trwa to nieco ponad 3 minuty. Ominiemy zatem wykonywanie i wczytamy 
obiekt z modelem z zewnętrznego pliku

```{r}
if (0) {
  set.seed(123456789)
  daneusa.gbm2  <- train(model1.formula,
                         data = daneusa.train2,
                         distribution = "bernoulli",
                         method = "gbm",
                         tuneGrid = parametry_gbm,
                         trControl = ctrl_cv3,
                         verbose = FALSE)
  saveRDS(object = daneusa.gbm2,
          file   = here("output", "daneusa.gbm2.rds"))
}
daneusa.gbm2 <- readRDS(here("output", "daneusa.gbm2.rds"))
```

Najlepszy zestaw to:

```{r}
daneusa.gbm2
```

A zatem: 

* `n.trees` = 500, 
* `interaction.depth` = 4,
* `shrinkage` = 0.1 
* `n.minobsinnode` = 100

Oceńmy teraz błąd prognozy na próbie uczącej i testowej.

Próba ucząca:

```{r}
daneusa.pred.train.gbm2 <- predict(daneusa.gbm2,
                                  daneusa.train, 
                                  # type = "response" daje 
                                  # w tym przypadku
                                  # prawdopodobieństwo sukcesu
                                  type = "prob",
                                  # parametr n.trees umożliwia
                                  # wybranie iteracji, z której 
                                  # prognozę zwracamy
                                  n.trees = 500)

getAccuracyAndGini2(data = data.frame(UCURNINS = daneusa.train$UCURNINS,
                                      pred = daneusa.pred.train.gbm2[, "Yes"]),
                    predicted_probs = "pred",
                    target_variable = "UCURNINS") 
```

Próba testowa:

```{r}
daneusa.pred.test.gbm2 <- predict(daneusa.gbm2,
                                  daneusa.test, 
                                  # type = "response" daje 
                                  # w tym przypadku
                                  # prawdopodobieństwo sukcesu
                                  type = "prob",
                                  # parametr n.trees umożliwia
                                  # wybranie iteracji, z której 
                                  # prognozę zwracamy
                                  n.trees = 500)

getAccuracyAndGini2(data = data.frame(UCURNINS = daneusa.test$UCURNINS,
                                      pred = daneusa.pred.test.gbm2[, "Yes"]),
                    predicted_probs = "pred",
                    target_variable = "UCURNINS") 
```


Przedstawmy jeszcze krzywe ROC na wykresie:

```{r}
ROC.train.gbm2 <- pROC::roc(daneusa.train$UCURNINS, 
                            daneusa.pred.train.gbm2[, "Yes"])
ROC.test.gbm2  <- pROC::roc(daneusa.test$UCURNINS, 
                            daneusa.pred.test.gbm2[, "Yes"])
cat("AUC for train = ", pROC::auc(ROC.train.gbm2), 
    ", Gini for train = ", 2 * pROC::auc(ROC.train.gbm2) - 1, "\n", sep = "")
cat("AUC for test = ", pROC::auc(ROC.test.gbm2), 
    ", Gini for test = ", 2 * pROC::auc(ROC.test.gbm2) - 1, "\n", sep = "")
plot(ROC.train.gbm2)
lines(ROC.test.gbm2, col = "red")
```

I raz jeszcze za pomoca funkcji `ggroc()`

```{r}
list(
  ROC.train.gbm2 = ROC.train.gbm2,
  ROC.test.gbm2  = ROC.test.gbm2
) %>%
  pROC::ggroc(alpha = 0.5, linetype = 1, size = 1) + 
  geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1), 
               color = "grey", 
               linetype = "dashed") +
  labs(subtitle = paste0("Gini TRAIN: ",
                         "gbm2 = ", 
                         round(100 * (2 * auc(ROC.train.gbm2) - 1), 1), "%, ",
                         "Gini TEST: ",
                         "gbm2 = ", 
                         round(100 * (2 * auc(ROC.test.gbm2) - 1), 1), "%, "
  )) +
  theme_bw() + coord_fixed() +
  scale_color_brewer(palette = "Paired")
```

GBM bez tuningu był lepszy - może dlatego, że był uczony na całej 
próbie treningowej?

Tu zresztą warto rozważyć więcej parametrów, zwłaszcza, że wyszły nam wartości 
graniczne jako optymalne. Ale to już jako praca domowa :-)

# Przykład 4.2 algorytm XGBOOST, zbiór `daneusa`

```{r}
# ################################################
# Przykład 4.2 
# zbiór daneusa
# XGBoost
# ################################################
```

Spróbujemy teraz z alternatywnym algorytmem treningowym, którym będzie 
**eXtreme Gradient Boosting**. Algorytm ten jest bardzo popularny w ostatnim 
okresie i jednocześnie jest dość szybki i skuteczny.

Skorzystamy z pakietu `xgboost`.

Najpierw ustalimy parametry samej walidacji 

```{r}
modelLookup("xgbTree")
```

Następnie zdecydujemy się na zestaw wartości parametrów do tuningu modelu XGBoost.
Mamy ich dwa rodzaje: związane z wielkością drzewa oraz z algoirytmem boostingu.

```{r}
# 1. nrounds - liczba iteracji do momentu zakończenia trenowania modelu
# 2. max_depth - maksymalna głębokość drzewa
#    zakres: [1, nieskończoność]
# 3. eta - wsp. learning rate
#    dla wysokich wartości eta trenowanie modelu odbywa się szybciej!
#    zakres: [0, 1]
#    Z kolei niskie wartości dają lepsze wyniki, pod warunkiem, że 
#    algorytm jest trenowany na odpowiednio dużej liczbie drzew.
#    To jednak (znacznie) zwiększa jego złożoność obliczeniową i czasową
# 4. gamma - Minimum Loss Reduction, minimalna redukcja optymizowanej funkcji 
#    celu wymagana do wykonania kolejnego podziału w bieżącym końcowym 
#    węźle drzewa. Wysoka wartość oznacza bardziej konserwatywny model.
#    Można próbować z różnymi wartościami tego parametru jednak 
#    za optymalizację modelu odpowiadają najczęściej inne parametry
#    zakres: [0, nieskończoność]
# 5. colsample_bytree - odsetek predyktorów wykorzystywanych w szukaniu
#    optymalnych podziałów (podobnie jak w lasach losowych). 
#    Wysokie wartości mogą prowadzić do przeuczenia modelu.
#    Niskie wartości mogą prowadzić do niższej dokładności modelu.
#    Zaleca się manipulowanie tym parametrem.
#    zakres: (0, 1]
# 6. min_child_weight - interpretacja podobna do minimalnej liczebności
#    obserwacji w węźle końcowym
#    zakres: [0, nieskończoność]
# 7. subsample - wielkośc podpróbki losowanej ze zbioru treningowego
#    wykorzystywanej do trenowania modelu. 
#    1 oznacza 100% (całość zbioru treningowego)
#    typowe wartości: [0.5, 1]
```


Sensowny algorytm tuningu parametrów dla `xgboost` jest następujący:


1. Wybierz względnie dużą wartość *learning rate*. Domyślną jest 0.1, ale równie dobrze można zacząć od wartości między 0.05 i 0.2 (zależnie od problemu).
2. Wyznacz optymalną liczbę drzew dla tej wartości *learning rate*. Zwykle jest to wartość ok. 40-70. 
   Uwaga! Wybierz wartość, dla której Twój komputer policzy wynik wystarczająco szybko, 
   ponieważ te parametry będą używane do testowania różnych parametrów wielkości drzewa.
3. Dokonaj strojenia parametrów związanych z wielkością drzewa dla wybranej *learning rate*
   i wielkości drzewa. 
4. Obniż wartość *learning rate* i zwiększ odpowiednio liczbę drzew, aby uzyskać  stabilniejszy model.

Aby ustalić optymalne parametry boostingu, musimy przyjąć jakieś początkowe 
wartości parametrów drzew.

Przyjmijmy zatem:

* `min_child_weight = 150`, powinno być ~0.5-1% liczby obserwacji. Dla danych niezbilansowanych lepiej dolnej granicy.
* `max_depth = 8`, warto wybrać (5-8) zależnie od liczby obserwacji i zmiennych (większe dla większych danych).
* `colsample_bytree = sqrt(p)/p`, tutaj mamy regułę kciuka - zacznij od pierwiastka liczby wszystkich zmiennych - weźmy zatem 6/30.
* `subsample = 0.8`, często używana wartość początkowa
* `eta = 0.25`, to jest własnie *learning rate*

```{r}
parametry_xgb <- expand.grid(nrounds = seq(20, 80, 10),
                             max_depth = c(8),
                             eta = c(0.25), 
                             gamma = 1,
                             colsample_bytree = c(0.2),
                             min_child_weight = c(150),
                             subsample = 0.8)
```

Poziomy zmiennych jakościowych należy przekształcić, aby nie zaczynały się 
od cyfry. Jest to wymagane przez `method = "xgbTree"` w funkcji `train()`.

```{r}
set.seed(123456789)
daneusa.xgb <- train(model1.formula,
                     data = daneusa.train,
                     method = "xgbTree",
                     trControl = ctrl_cv3,
                     tuneGrid  = parametry_xgb)
```

Podsumowanie walidacji krzyżowej

```{r}
daneusa.xgb
```

Od razu zapiszemy model do pliku `rds`

```{r}
daneusa.xgb %>% saveRDS(here("output", "daneusa.xgb.rds"))
```


Najlepszy model uzyskano dla `nrounds = 80`. Nie sprawdzajmy już wyższych

Generalnie:
1. jeśli wartość jest niska (20), można zmniejszyć `eta` do 0.05 i spróbować jeszcze raz
2. jeśli wartość jest wysoka (100+) tuning pozostałych parametrów będzie długi - można wybrać wyższą `eta` i spróbować ponownie

Następnym krokiem jest znalezienie optymalnych wartości parametrów drzewa:

- `max_depth`
- `min_child_weight`
- `colsample_bytree`

Zaczniemy od dwóch pierwszych, bo mają większy wpływ na wynikowe drzewo:

```{r}
parametry_xgb2 <- expand.grid(nrounds = 80,
                              max_depth = seq(5, 15, 2),
                              eta = c(0.25), 
                              gamma = 1,
                              colsample_bytree = c(0.2),
                              min_child_weight = seq(200, 1000, 200),
                              subsample = 0.8)

set.seed(123456789)
daneusa.xgb2 <- train(model1.formula,
                      data = daneusa.train,
                      method = "xgbTree",
                      trControl = ctrl_cv3,
                      tuneGrid  = parametry_xgb2)
```

Trwa to kilkanaście sekund. Wyniki:

```{r}
daneusa.xgb2
```

Od razu zapiszemy model do pliku `rds`.

```{r}
daneusa.xgb2 %>% saveRDS(here("output", "daneusa.xgb2.rds"))
```

Mamy zatem `maxdepth = 9`, `min_child_weight = 200`

Teraz znajdźmy ostatnie parametry dla drzewa. 

Najpierw `colsample_bytree`

```{r}
parametry_xgb3 <- expand.grid(nrounds = 80,
                              max_depth = 9,
                              eta = c(0.25), 
                              gamma = 1,
                              colsample_bytree = seq(0.1, 0.8, 0.1),
                              min_child_weight = 200,
                              subsample = 0.8)
set.seed(123456789)
daneusa.xgb3 <- train(model1.formula,
                      data = daneusa.train,
                      method = "xgbTree",
                      trControl = ctrl_cv3,
                      tuneGrid  = parametry_xgb3)
```

Znów trwa to kilkanaście sekund.

```{r}
daneusa.xgb3
```

Zapisujemy model do pliku `rds`.

```{r}
daneusa.xgb3 %>% saveRDS(here("output", "daneusa.xgb3.rds"))
```

A zatem `colsample_bytree = 0.7`.

Następnym krokiem będzie sprawdzenie optymalnej wielkości próbki, wykorzystywanej
przy tworzeniu podziałów. Odpowiada za to argument `subsample`. 
Wypróbujmy następujące wartości: 0.6, 0.7, 0.75, 0.8, 0.85, 0.9.

```{r}
parametry_xgb4 <- expand.grid(nrounds = 80,
                              max_depth = 9,
                              eta = c(0.25), 
                              gamma = 1,
                              colsample_bytree = 0.7,
                              min_child_weight = 200,
                              subsample = c(0.6, 0.7, 0.75, 0.8, 0.85, 0.9))

set.seed(123456789)
daneusa.xgb4 <- train(model1.formula,
                      data = daneusa.train,
                      method = "xgbTree",
                      trControl = ctrl_cv3,
                      tuneGrid  = parametry_xgb4)
daneusa.xgb4
```

Optymalna wartość to `subsample = 0.9`.

Tu także zapisujemy model do pliku `rds`.

```{r}
daneusa.xgb4 %>% saveRDS(here("output", "daneusa.xgb4.rds"))
```

Mamy zatem wszystkie potrzebne wartości parametrów. 
Uwaga! Niekoniecznie są one "najbardziej" optymalne, ale stanowią dobry benchmark.
Teraz obniżymy `learning rate` i proporcjonalnie zwiększymy liczbę drzew.

Zwiększenie liczby drzew zwiększa koszty obliczeń (w tym walidacji krzyżowej).

Zmniejszmy *learning rate* o połowę (do 0.12), zaś liczbę drzew zwiększmy 
dwukrotnie (do 160).

```{r}
parametry_xgb5 <- expand.grid(nrounds = 160,
                              max_depth = 9,
                              eta = 0.12, 
                              gamma = 1,
                              colsample_bytree = 0.7,
                              min_child_weight = 200,
                              subsample = 0.9)

set.seed(123456789)
daneusa.xgb5 <- train(model1.formula,
                      data = daneusa.train,
                      method = "xgbTree",
                      trControl = ctrl_cv3,
                      tuneGrid  = parametry_xgb5)
daneusa.xgb5
```

Tu także zapisujemy model do pliku `rds`.

```{r}
daneusa.xgb5 %>% saveRDS(here("output", "daneusa.xgb5.rds"))
```

Kolejna korekta `eta` i liczby drzew:

```{r}
parametry_xgb6 <- expand.grid(nrounds = 320,
                              max_depth = 9,
                              eta = 0.06, 
                              gamma = 1,
                              colsample_bytree = 0.7,
                              min_child_weight = 200,
                              subsample = 0.9)

set.seed(123456789)
daneusa.xgb6 <- train(model1.formula,
                      data = daneusa.train,
                      method = "xgbTree",
                      trControl = ctrl_cv3,
                      tuneGrid  = parametry_xgb6)
daneusa.xgb6
```

I tradycyjnie zapiszemy model do pliku `rds`

```{r}
daneusa.xgb6 %>% saveRDS(here("output", "daneusa.xgb6.rds"))
```

Można dalej obniższać eta i proporcjonalnie zwiększać liczbę drzew.

Porównajmy oszacowane dotąd modele na próbie testowej.

```{r}
source(here("funs", "getAccuracyAndGini.R"))
(modele <- c("", "2":"6"))
sapply(paste0("daneusa.xgb", modele),
       function(x) getAccuracyAndGini(model = get(x),
                                      data = daneusa.test,
                                      target_variable = "UCURNINS",
                                      predicted_class = "Yes")
)
```
 
Gini w ostatnim modelu: ~= 68.85% i jest nieco lepszy niż w GBM.

```{r}
ROC.train <- pROC::roc(daneusa.train$UCURNINS, 
                       predict(daneusa.xgb6,
                               daneusa.train, type = "prob")[, "Yes"])
ROC.test  <- pROC::roc(daneusa.test$UCURNINS, 
                       predict(daneusa.xgb6,
                               daneusa.test, type = "prob")[, "Yes"])
cat("AUC for train = ", pROC::auc(ROC.train), 
    ", Gini for train = ", 2 * pROC::auc(ROC.train) - 1, "\n", sep = "")
cat("AUC for test = ", pROC::auc(ROC.test), 
    ", Gini for test = ", 2 * pROC::auc(ROC.test) - 1, "\n", sep = "")
```

I na koniec wspólny wykres ROC.

Najpierw wartości ROC:

```{r}
ROC.train.xgb <- pROC::roc(daneusa.train$UCURNINS, 
                           predict(daneusa.xgb2,
                                   daneusa.train, 
                                   type = "prob",
                                   n.trees = 500)[, "Yes"])
ROC.test.xgb  <- pROC::roc(daneusa.test$UCURNINS, 
                           predict(daneusa.xgb2,
                                   daneusa.test, 
                                   type = "prob",
                                   n.trees = 500)[, "Yes"])
ROC.train.xgb6 <- pROC::roc(daneusa.train$UCURNINS, 
                           predict(daneusa.xgb6,
                                   daneusa.train, 
                                   type = "prob",
                                   n.trees = 500)[, "Yes"])
ROC.test.xgb6  <- pROC::roc(daneusa.test$UCURNINS, 
                           predict(daneusa.xgb6,
                                   daneusa.test, 
                                   type = "prob",
                                   n.trees = 500)[, "Yes"])
```

A następnie zbieramy wszystko razem do listy i stosujemy funkcję `ggroc()`

```{r}
list(
  ROC.train.gbm   = ROC.train.gbm,
  ROC.test.gbm    = ROC.test.gbm,
  ROC.train.xgb  = ROC.train.xgb,
  ROC.test.xgb   = ROC.test.xgb,
  ROC.train.xgb6  = ROC.train.xgb6,
  ROC.test.xgb6   = ROC.test.xgb6
) %>%
  pROC::ggroc(alpha = 0.5, linetype = 1, size = 1) + 
  geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1), 
               color = "grey", 
               linetype = "dashed") +
  labs(subtitle = paste0("Gini TRAIN: ",
                         "gbm = ", 
                         round(100 * (2 * auc(ROC.train.gbm) - 1), 1), "%, ",
                         "xgb = ", 
                         round(100 * (2 * auc(ROC.train.xgb) - 1), 1), "%, ",
                         "xgb6 = ", 
                         round(100 * (2 * auc(ROC.train.xgb6) - 1), 1), "%, ",
                         "Gini TEST: ",
                         "gbm = ", 
                         round(100 * (2 * auc(ROC.test.gbm) - 1), 1), "%, ",
                         "xgb = ", 
                         round(100 * (2 * auc(ROC.test.xgb) - 1), 1), "%, ",
                         "xgb6 = ", 
                         round(100 * (2 * auc(ROC.test.xgb6) - 1), 1), "% ")) +
  theme_bw() + coord_fixed() +
  scale_color_brewer(palette = "Paired")
```

Najlepszy na zbiorze testowym jest xgb6, ale należy zauważyć, że jest też
najbardziej przeuczony, a zatem prawdopodobnie będzie się charakteryzować
dużym bledem ex-ante w probie OUT-OF-TIME (tj. wtedy, gdy model zostanie
zastosowany do danych, które dopiero pojawią się w czasie).


# Przykład 4.3 algorytm Adaboost, zbiór `daneusa`

```{r}
# ######################################################
# Przykład 4.3 
# zbiór daneusa
# Adaboost
# ######################################################
```

Zdecydujemy się na zestaw wartości parametrów do tuningu modelu Adaboost.

```{r}
modelLookup("adaboost")
```

1. `nIter` - liczba iteracji (drzew) do momentu zakończenia trenowania modelu
2. `method` - metoda adaboost
    * `Adaboost.M1` - algorytm ustalający wagi dla kolejnych iteracji, bazuje na przewidywanych kategoriach zmiennej celu,
    * `Real adaboost` - jest uogólnieniem i poprawką `AdaBoost.M1`, która wykorzystuje przewidywane prawdopodobieństwa kategorii


```{r}
parametry_adaboost <- expand.grid(nIter = c(100, 200),
                                  method = c("Adaboost.M1",
                                             "Real adaboost"))
```

UWAGA! Nawet na mniejszym zbiorze treningowym tuning parametrów trwa ok. 46 minut, 
a wynikowy obiekt ma ok. 1 GB! To dlatego, że wynik przechowuje wszystkie kolejne
obiekty `rpart` (których jest mieszanką), a każdy z nich przechowuje dużo informacji

Ominiemy zatem wykowananie poniższego kodu:

```{r savingAdaResult}
if (0) {
  set.seed(123456789)
  daneusa.ada <- train(model1.formula,
                       data = daneusa.train,
                       method = "adaboost",
                       trControl = ctrl_cv3,
                       tuneGrid  = parametry_adaboost)
  daneusa.ada %>% saveRDS(here("output", "daneusa.ada.rds"))
}
```

I wczytamy obiekt z wynikowym modelem z przygotowanego już wcześniej
zewnętrznego pliku:

```{r readingAdaResult}
daneusa.ada <- readRDS(here("output", "daneusa.ada.rds"))
```

UWAGA!Predykcja także trwa długo.

```{r predictingAda}
daneusa.pred.train.ada <- predict(daneusa.ada,
                                  daneusa.train, 
                                  type = "prob")
daneusa.pred.test.ada <- predict(daneusa.ada,
                                 daneusa.test, 
                                 type = "prob")
```

Przedstawmy jeszcze krzywe ROC na wykresie.

```{r}
ROC.train.ada <- pROC::roc(daneusa.train$UCURNINS == "Yes", 
                           daneusa.pred.train.ada[, "Yes"])
ROC.test.ada  <- pROC::roc(daneusa.test$UCURNINS == "Yes", 
                           daneusa.pred.test.ada[, "Yes"])
cat("AUC for train = ", pROC::auc(ROC.train.ada), 
    ", Gini for train = ", 2 * pROC::auc(ROC.train.ada) - 1, "\n", sep = "")
cat("AUC for test = ", pROC::auc(ROC.test.ada), 
    ", Gini for test = ", 2 * pROC::auc(ROC.test.ada) - 1, "\n", sep = "")

list(
  ROC.train.ada = ROC.train.ada,
  ROC.test.ada  = ROC.test.ada
) %>%
  pROC::ggroc(alpha = 0.5, linetype = 1, size = 1) + 
  geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1), 
               color = "grey", 
               linetype = "dashed") +
  labs(subtitle = paste0("Gini TRAIN: ",
                         "ada = ", 
                         round(100 * (2 * pROC::auc(ROC.train.ada) - 1), 1), "%, ",
                         "Gini TEST: ",
                         "ada = ", 
                         round(100 * (2 * pROC::auc(ROC.test.ada) - 1), 1), "%, "
  )) +
  theme_bw() + coord_fixed() +
  scale_color_brewer(palette = "Paired")
```

Widzimy zatem, że model jest mocno przeuczony, zaś na zbiorze testowym
wypada słabiej od modelu xbgoost.

Wykorzystajmy alternatywnie algorytm `adaStump` (stump = pień), który zawsze 
działa na drzewach o głębokości 1 
(odpowiednik `ada()` z `maxdepth = 1`, `cp = -1` i `minsplit = 0`)

```{r}
daneusa.adastump <- 
  adaStump(formula = model1.formula,
           # weźmy cały zbiór treningowy
           data = daneusa.train, 
           # typ algorytmu boostingu
           # type = "real", 
           # type = "discrete",
           type = "gentle",
           # liczba iteracji boostowanych
           iter = 200, # domyślnie 50
           # shrinkage parameter (learning rate)
           # domyślnie 1
           nu = 0.1, 
           # frakcja baggingowa - 
           # jaka część zbioru wykorzystana
           # w kolejnych iteracjach
           bag.frac = 0.8)
```

Trwa to zdecydowanie krócej!

Oceńmy błąd prognozy na próbie uczącej i testowej.

```{r}
daneusa.pred.train.adastump <- predict(daneusa.adastump, daneusa.train)
daneusa.pred.test.adastump  <- predict(daneusa.adastump, daneusa.test)
```

próba ucząca
```{r}
getAccuracyAndGini2(data = data.frame(UCURNINS = daneusa.train$UCURNINS,
                                      pred = daneusa.pred.train.adastump),
                    predicted_probs = "pred",
                    target_variable = "UCURNINS") 
```

próba testowa
```{r}
getAccuracyAndGini2(data = data.frame(UCURNINS = daneusa.test$UCURNINS,
                                      pred = daneusa.pred.test.adastump),
                    predicted_probs = "pred",
                    target_variable = "UCURNINS") 
```

Niestety, adastump wypada gorzej...

```{r}
ROC.train.adastump <- pROC::roc(daneusa.train$UCURNINS, 
                                daneusa.pred.train.adastump)
ROC.test.adastump  <- pROC::roc(daneusa.test$UCURNINS, 
                                daneusa.pred.test.adastump)
cat("AUC for train = ", pROC::auc(ROC.train.adastump), 
    ", Gini for train = ", 2 * pROC::auc(ROC.train.adastump) - 1, "\n", sep = "")
cat("AUC for test = ", pROC::auc(ROC.test.adastump), 
    ", Gini for test = ", 2 * pROC::auc(ROC.test.adastump) - 1, "\n", sep = "")

list(
  ROC.train.adastump = ROC.train.adastump,
  ROC.test.adastump = ROC.test.adastump) %>%
  pROC::ggroc(alpha = 0.5, linetype = 1, size = 1) + 
  geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1), 
               color = "grey", 
               linetype = "dashed") +
  labs(subtitle = paste0("Gini TRAIN: ",
                         "adastump = ", 
                         round(100 * (2 * auc(ROC.train.adastump) - 1), 1), "%, ",
                         "Gini TEST: ",
                         "adastump = ", 
                         round(100 * (2 * auc(ROC.test.adastump) - 1), 1), "%, ")) +
  theme_bw() + coord_fixed() +
  scale_color_brewer(palette = "Paired")
```


Warto zatem spróbować z innymi parametrami!

Inne ciekawe przykłady zastosowania algorymtów gbm i xgboost:

* https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/
*  https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/


# Przykład 4.4 algorytm catboost, zbiór `boston`

```{r}
# ##############################################################
# Przykład 4.4 
# BOOSTING drzew decyzyjnych
# zbiór boston, pakiet catboost
# ##############################################################
```

Catboost jest nowoczesnym algorytem rosyjskiej firmy Yandex. Wywodzi się od
xgboosta, lecz m. in. inaczej przetwarza cechy jakościowe, „generując losowe permutacje zestawu danych i dla każdej próbki obliczając średnią wartość etykiety dla próbki o tej samej wartości kategorii umieszczonej przed daną w permutacji”. Więcej informacji o catboost można znaleźć tutaj:

* https://catboost.ai/  
* https://www.analyticsvidhya.com/blog/2017/08/catboost-automated-categorical-data/

Instalowanie `catboost` na komputerze z MS Windows

```{r}
# wersja 0.23.2, 
# dla najnowszej wersji proszę sprawdzić numer i scieżkę na stronie:
# https://github.com/catboost/catboost/releases

if (0) {
  devtools::install_url('https://github.com/catboost/catboost/releases/download/v0.23.2/catboost-R-Windows-0.23.2.tgz',
  INSTALL_opts = c("--no-multiarch"))
}

```

Po instalacji ładujemy bibliotekę do pamięci:

```{r}
library(catboost)
library(tidyverse)
library(caret)
```

Ładujemy także zbiór `boston`

```{r}
Boston <- MASS::Boston %>% as_tibble()
glimpse(Boston)
```

I tradycyjnie dokonujemy podziału na próbkę treningową i testową

```{r}
set.seed(123456789)
training_obs <- createDataPartition(Boston$medv, 
                                    p = 0.7, 
                                    list = FALSE) 
Boston.train <- Boston[training_obs,]
Boston.test  <- Boston[-training_obs,]
```

Nastęþnie budujemy obiekt ze zbiórem traningowy w postaci wymaganej przez 
funkcję `catboost.train()`

```{r}
train_data    <- Boston.train[, 1:13] %>% as.data.frame()
train_targets <- Boston.train[, 14] %>% pull()
train_pool    <- catboost.load_pool(data = train_data, label = train_targets)
```

Przygotowujemy także listę z parametrami

```{r}
params <- list(iterations = 1000,
               learning_rate = 0.075,
               depth = 3,
               loss_function = 'RMSE',
               eval_metric = 'RMSE',
               random_seed = 560,
               od_type = 'Iter',
               metric_period = 50,
               od_wait = 10,
               use_best_model = F)
```


Trenowanie modelu odbywa się poprzez wywołanie funkcji `catboost.train()`.

```{r}
model <- catboost.train(train_pool, params = params)
```

Tworzymy zbiór testowy:

```{r}
test_data    <- Boston.test[, 1:13] %>% as.data.frame()
test_targets <- Boston.test[, 14] %>% pull()
real_pool    <- catboost.load_pool(test_data)
```

Obliczamy predykcje:

```{r}
prediction <- catboost.predict(model, real_pool)
```

Następnie możemy porównać predykcje z wartościami faktycznymi na zbiorze testowym

```{r}
tibble(
  pred = prediction,
  actual = test_targets
) %>% 
  ggplot(aes(pred, actual)) +
  geom_point() +
  geom_smooth(method = "lm")
```


Obliczymy także miary jakości prognozy

```{r}
tibble(
  pred = prediction,
  actual = test_targets
) %>%
  mutate(ae = abs(pred - actual),
         se = (pred - actual) ^ 2) %>%
  summarize(mae = mean(ae),
            mse = mean(se))
```

Dla porównania inne modele

```{r}
source(here("scripts", "88_exmpl_boston_models.R"))
mean((boston.lm.pred     - Boston.test$medv) ^ 2)
mean((boston.tree.pred   - Boston.test$medv) ^ 2)
mean((boston.pruned.pred - Boston.test$medv) ^ 2)
mean((boston.bag.pred    - Boston.test$medv) ^ 2)
mean((boston.rf.pred     - Boston.test$medv) ^ 2)
mean((boston.gbm.pred    - Boston.test$medv) ^ 2)
mean((boston.gbm2.pred   - Boston.test$medv) ^ 2)
mean((boston.gbm3.pred   - Boston.test$medv) ^ 2)

```

I na tkoniec wykres ważności predyktorów

```{r}
fi <- catboost.get_feature_importance(model, 
                                      pool = NULL, 
                                      type = 'FeatureImportance',
                                      thread_count = -1) 
tibble(
  feat = rownames(fi) %>% as.factor(),
  imp  = fi[, 1]
) %>% 
  ggplot(aes(reorder(feat, imp), imp)) +
  geom_bar(stat = "identity") + 
  coord_flip()
```

# Ćwiczenia #4

## Ćwiczenie 4.1 {.tabset .tabset-fade}
### Polecenie

Zastosuj różne metody boostingu dla danych `titanic`/`churn`.
Zoptymalizuj parametry za pomocą walidacji krzyżowej.
Porównaj jakość uzyskanych prognoz z wartościami
dla lasów losowych.

### Przykładowe rozwiązanie

```{r r, code = readLines("exrcs/exrcs_4_1.R")}
```


## Ćwiczenie 4.2 {.tabset .tabset-fade}
### Polecenie

Zastosuj różne metody boostingu dla danych `boston`/`bikes`.
Zoptymalizuj parametry za pomocą walidacji krzyżowej.
Porównaj jakość uzyskanych prognoz z wartościami dla lasów losowych.

### Przykładowe rozwiązanie

```{r r, code = readLines("exrcs/exrcs_4_2.R")}
```


## Ćwiczenie 4.3 {.tabset .tabset-fade}
### Polecenie

Zastosuj algorytm catboost dla problemu regresji na danych `boston`/`bikes`.
Zoptymalizuj parametry za pomocą walidacji krzyżowej.
Porównaj jakość uzyskanych prognoz z wartościami dla poprzednich modeli.

### Przykładowe rozwiązanie

```{r r, code = readLines("exrcs/exrcs_4_3.R")}
```


## Ćwiczenie 4.4 {.tabset .tabset-fade}
### Polecenie

Zastosuj algorytm catboost dla problemu klasyfikacji na danych `titanic`/`churn`.
Zoptymalizuj parametry za pomocą walidacji krzyżowej.
Porównaj jakość uzyskanych prognoz z wartościami dla poprzednich modeli.

### Przykładowe rozwiązanie

```{r r, code = readLines("exrcs/exrcs_4_4.R")}
```




